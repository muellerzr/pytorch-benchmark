# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_utils.ipynb (unless otherwise specified).

__all__ = ['get_rank', 'get_device', 'get_process_index', 'num_processes']

# Cell
from .imports import is_tpu_available, is_multigpu_available
import os
import torch

if is_tpu_available(False):
    import torch_xla.core.xla_model as xm

# Cell
def get_rank() -> int:
    "Gets the local rank"
    return int(os.environ.get("LOCAL_RANK", -1))

# Cell
def get_device():
    if is_tpu_available():
        return xm.xla_device()
    elif is_multigpu_available():
        device = torch.device("cuda", get_rank())
        torch.cuda.set_device(device)
        return device
    else:
        return torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Cell
def get_process_index():
    "Gets the local process index"
    if is_tpu_available():
        return xm.get_local_ordinal()
    elif is_multigpu_available():
        return get_rank()
    return 0

# Cell
def num_processes() -> int:
    "Returns the number of processes in the distributed system"
    if is_tpu_available():
        return xm.xrt_world_size()
    elif is_multigpu_available():
        return torch.distributed.get_world_size()
    return 1